{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instructional-cameroon",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/epsilon-deltta/amazon-review/blob/master/eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "still-party",
   "metadata": {
    "id": "exact-notebook"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "engaging-opera",
   "metadata": {
    "id": "prescribed-ghana"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-contemporary",
   "metadata": {
    "id": "institutional-workplace"
   },
   "source": [
    "1. Training Set Construction (5 pts)\n",
    "Construct the training set for the amazon review dataset as instructed and report the following statistics.\n",
    "Statistics\n",
    "\n",
    "- the total number of unique words in T\n",
    "    : 22764 #len(token.word_index)\n",
    "- the total number of training examples in T  \n",
    "    : 2000  #len(df)\n",
    "- the ratio of positive examples to negative examples in T   \n",
    "    : 1:1   #len(poslist)/len(neglist)\n",
    "- the average length of document in T   \n",
    "    : 168.915\n",
    "- the max length of document in T\n",
    "    : 3394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-edgar",
   "metadata": {
    "id": "graduate-investment"
   },
   "outputs": [],
   "source": [
    "# df,tedf = load_data()\n",
    "def load_data():\n",
    "    pass\n",
    "def preprocess(review):\n",
    "    pass\n",
    "# df.review = preprocess(df.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-being",
   "metadata": {
    "id": "prospective-transparency"
   },
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-video",
   "metadata": {
    "id": "continuous-ozone"
   },
   "outputs": [],
   "source": [
    "# in colab\n",
    "!git clone https://github.com/epsilon-deltta/dataset data\n",
    "path = './data/amazon-review/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quiet-exposure",
   "metadata": {
    "id": "ethical-harbor"
   },
   "outputs": [],
   "source": [
    "# in local\n",
    "path     = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demanding-substance",
   "metadata": {
    "id": "ethical-harbor"
   },
   "outputs": [],
   "source": [
    "def load_data(path='./data/'):\n",
    "    testdir  = path + 'test/'\n",
    "    traindir = path + 'train/'\n",
    "\n",
    "    df = [ pd.DataFrame(columns = ['n','review','target']) for _ in range(2)]\n",
    "\n",
    "    for i,path in enumerate([traindir,testdir]):\n",
    "        path = path\n",
    "        label = 'positive'\n",
    "\n",
    "        poslist = [ os.path.join(path,label,name) for name in os.listdir(path+label) ]\n",
    "\n",
    "        label = 'negative' \n",
    "        neglist = [ os.path.join(path,label,name) for name in os.listdir(path+label) ]\n",
    "\n",
    "        sep = os.path.sep\n",
    "\n",
    "        for fpath in poslist :\n",
    "            num = fpath.split(sep)[-1]\n",
    "            with open(fpath,'r') as f :\n",
    "                content = f.read()\n",
    "\n",
    "            item = dict(zip(df[i].columns,[num,content,1] ) )\n",
    "            df[i] = df[i].append(item,ignore_index=True)\n",
    "\n",
    "        for fpath in neglist :\n",
    "            num = fpath.split(sep)[-1]\n",
    "            with open(fpath,'r') as f :\n",
    "                content = f.read()\n",
    "\n",
    "            item = dict(zip(df[i].columns,[num,content,0] ) )\n",
    "            df[i] = df[i].append(item,ignore_index=True)\n",
    "    return df[0].review,df[0].target,df[1].review,df[1].target\n",
    "\n",
    "xtr,ytr,xte,yte = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-melbourne",
   "metadata": {
    "id": "expressed-abraham"
   },
   "source": [
    "#### count words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "starting-filter",
   "metadata": {
    "id": "higher-unemployment"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence,Tokenizer\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "from keras.utils import to_categorical as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "elegant-audit",
   "metadata": {
    "id": "stainless-growth"
   },
   "outputs": [],
   "source": [
    "token = Tokenizer()             # 토큰화 함수 지정\n",
    "token.fit_on_texts(xtr)       # 토큰화 함수에 문장 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-dating",
   "metadata": {
    "id": "clear-intake"
   },
   "outputs": [],
   "source": [
    "docs =  token.texts_to_sequences(xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-guyana",
   "metadata": {
    "id": "promotional-diploma"
   },
   "outputs": [],
   "source": [
    "# the average length of document in T\n",
    "counts = []\n",
    "for doc in docs:\n",
    "    counts.append(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-champion",
   "metadata": {
    "id": "celtic-protocol",
    "outputId": "be8b94d9-540b-4fb2-fdf8-c9f7b2d549d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168.915"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-cleaners",
   "metadata": {
    "id": "standing-hopkins"
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open('data/df.pkl','wb') as f:\n",
    "    pk.dump(df,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-divide",
   "metadata": {
    "id": "driving-silicon"
   },
   "source": [
    "2. Performance of deep neural network for classification (20 pts)\n",
    "Suggested hyperparameters: \n",
    "\t\n",
    "Data processing: \n",
    "- Word embedding dimension: 100\n",
    "- Word Index: keep the most frequent 10k words\n",
    "\n",
    "CNN\n",
    "- Network: Word embedding lookup layer -> 1D CNN layer -> fully connected layer -> output prediction\n",
    "- Number of filters: 100\n",
    "- Filter length: 3\n",
    "- CNN Activation: Relu\n",
    "- Fully connected layer dimension 100, activation: None (i.e. this layer is linear)\n",
    "\n",
    "RNN:\n",
    "- Network: Word embedding lookup layer -> LSTM layer -> fully connected layer(on the hidden state of the last LSTM cell) -> output prediction\n",
    "- Hidden dimension for LSTM cell: 100\n",
    "- Activation for LSTM cell: tanh\n",
    "- Fully connected layer dimension 100, activation: None (i.e. this layer is linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-terminology",
   "metadata": {
    "id": "quarterly-jumping",
    "outputId": "6fc0682b-e258-433f-850d-a290efbaacbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x1b3ff32c860>,\n",
       "  <matplotlib.lines.Line2D at 0x1b3ff32cb00>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x1b3ff32ce10>,\n",
       "  <matplotlib.lines.Line2D at 0x1b3ff33e128>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x1b3ff32c5f8>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x1b3ff33e400>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x1b3ff33e6d8>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-mercury",
   "metadata": {
    "id": "standing-donna",
    "outputId": "6eb976c4-4b06-436d-b187-0625a72d026c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>168.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>186.859878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3394.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  2000.000000\n",
       "mean    168.915000\n",
       "std     186.859878\n",
       "min       9.000000\n",
       "25%      65.000000\n",
       "50%     117.000000\n",
       "75%     201.000000\n",
       "max    3394.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(counts).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-tamil",
   "metadata": {
    "id": "cloudy-beverage",
    "outputId": "8bebe66c-be23-4c42-cc03-5a3d0f9b23aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "including ratio : 0.9255\n"
     ]
    }
   ],
   "source": [
    "size = 400\n",
    "total = len(counts)\n",
    "i = 0 \n",
    "for x in counts:\n",
    "    if x < size:\n",
    "        i += 1\n",
    "print(\"including ratio :\",i/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-reach",
   "metadata": {
    "id": "heated-candle"
   },
   "source": [
    "#### tokenizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forward-terminology",
   "metadata": {
    "id": "unsigned-coast"
   },
   "outputs": [],
   "source": [
    "# - Word Index: keep the most frequent 10k words\n",
    "vocab_size = 10000\n",
    "token = Tokenizer(num_words = vocab_size + 1) # 상위 10000개 단어만 사용\n",
    "\n",
    "token.fit_on_texts(xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "valid-dakota",
   "metadata": {
    "id": "novel-render"
   },
   "outputs": [],
   "source": [
    "max_len = 400\n",
    "def preprocess(docs,token,max_len=400):\n",
    "    docs = token.texts_to_sequences(docs)\n",
    "    \n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    docs = pad_sequences(docs, maxlen=max_len)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "xtr = preprocess(xtr,token)\n",
    "xte = preprocess(xte,token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-resistance",
   "metadata": {
    "id": "solar-value"
   },
   "source": [
    "Let me make Embedding Matrix !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-arrow",
   "metadata": {
    "id": "controlled-method"
   },
   "outputs": [],
   "source": [
    "# - Word embedding dimension: 100\n",
    "# vocab_size  = token.num_words # 10000+ 1 \n",
    "vector_size = 100  #100\n",
    "vocab_size = 10001\n",
    "\n",
    "def get_emb_mtr():\n",
    "    wv = pd.read_csv(path+'all.review.vec.txt',sep=' ',skiprows=1,header=None)\n",
    "\n",
    "    wv.set_index(0,inplace=True)\n",
    "    del wv[101]\n",
    "\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, vector_size))\n",
    "\n",
    "    def get_vector(word):\n",
    "        if word in wv.index:\n",
    "            return wv.loc[word]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    num = 0\n",
    "    word_index = [( i,word) for i,word in token.index_word.items()][:10000]\n",
    "    # not_used = []\n",
    "    for i,word in word_index: \n",
    "        temp = get_vector(word)\n",
    "        if temp is not None: \n",
    "\n",
    "            embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다.\n",
    "    #     else :\n",
    "    #         not_used.append(word)\n",
    "    #         num+=1\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = get_emb_mtr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-family",
   "metadata": {},
   "source": [
    "#### Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parliamentary-chocolate",
   "metadata": {
    "id": "optional-first"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten  \n",
    "\n",
    "from keras.layers import Conv2D,MaxPooling1D,MaxPooling2D ,Conv1D\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding , LSTM\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comic-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr = xtr.astype('float')\n",
    "xte = xte.astype('float')\n",
    "ytr = ytr.astype('float')\n",
    "yte = yte.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "affecting-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn w/o pre-emb\n",
    "model0 = Sequential()\n",
    "e = Embedding(vocab_size, vector_size)\n",
    "model0.add(e)\n",
    "model0.add(Conv1D(100, 3, padding='valid', activation='relu',strides=1))\n",
    "model0.add(Dense(100))\n",
    "model0.add(Dense(1,activation='sigmoid'))\n",
    "# model1.add(MaxPooling1D(pool_size=4))\n",
    "model0.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "spanish-input",
   "metadata": {
    "id": "medieval-component"
   },
   "outputs": [],
   "source": [
    "# cnn w/ pretrained model\n",
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vector_size, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model1.add(e)\n",
    "# model1.add(Flatten())\n",
    "# model.add(Dropout(0.5)) \n",
    "model1.add(Conv1D(100, 3, padding='valid', activation='relu',strides=1))\n",
    "model1.add(Dense(100))\n",
    "model1.add(Dense(1,activation='sigmoid'))\n",
    "# model1.add(MaxPooling1D(pool_size=4))\n",
    "# model1.add(LSTM(55))\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "detailed-russell",
   "metadata": {
    "id": "unlike-silver"
   },
   "outputs": [],
   "source": [
    "# lstm model w/ pre-emb\n",
    "model2 = Sequential()\n",
    "e = Embedding(vocab_size, vector_size, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model2.add(e)\n",
    "model2.add(LSTM(100,activation='tanh'))\n",
    "# model.add(Dropout(0.5))\n",
    "model2.add(Dense(100))\n",
    "model2.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "revolutionary-theta",
   "metadata": {
    "id": "unlike-silver"
   },
   "outputs": [],
   "source": [
    "# lstm model w/o pre-emb\n",
    "model3 = Sequential()\n",
    "e = Embedding(vocab_size, vector_size)\n",
    "model3.add(e)\n",
    "model3.add(LSTM(100,activation='tanh'))\n",
    "# model.add(Dropout(0.5))\n",
    "model3.add(Dense(100))\n",
    "model3.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "decreased-acceptance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6939 - acc: 0.5011\n",
      "Epoch 00001: val_loss improved from inf to 0.69296, saving model to ./model0/01-0.6930.hdf5\n",
      "20/20 [==============================] - 4s 205ms/step - loss: 0.6939 - acc: 0.5011 - val_loss: 0.6930 - val_acc: 0.5209\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6902 - acc: 0.5329\n",
      "Epoch 00002: val_loss improved from 0.69296 to 0.68868, saving model to ./model0/02-0.6887.hdf5\n",
      "20/20 [==============================] - 4s 185ms/step - loss: 0.6902 - acc: 0.5329 - val_loss: 0.6887 - val_acc: 0.5270\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6816 - acc: 0.5332\n",
      "Epoch 00003: val_loss improved from 0.68868 to 0.68691, saving model to ./model0/03-0.6869.hdf5\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 0.6816 - acc: 0.5332 - val_loss: 0.6869 - val_acc: 0.5301\n",
      "Epoch 1/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6923 - acc: 0.4974\n",
      "Epoch 00001: val_loss improved from inf to 0.69758, saving model to ./model1/01-0.6976.hdf5\n",
      "20/20 [==============================] - 3s 155ms/step - loss: 0.6923 - acc: 0.4974 - val_loss: 0.6976 - val_acc: 0.5084\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6914 - acc: 0.5204\n",
      "Epoch 00002: val_loss improved from 0.69758 to 0.68828, saving model to ./model1/02-0.6883.hdf5\n",
      "20/20 [==============================] - 3s 143ms/step - loss: 0.6914 - acc: 0.5204 - val_loss: 0.6883 - val_acc: 0.5257\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6860 - acc: 0.5332\n",
      "Epoch 00003: val_loss improved from 0.68828 to 0.68581, saving model to ./model1/03-0.6858.hdf5\n",
      "20/20 [==============================] - 3s 146ms/step - loss: 0.6860 - acc: 0.5332 - val_loss: 0.6858 - val_acc: 0.5317\n",
      "Epoch 1/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6563 - acc: 0.6210\n",
      "Epoch 00001: val_loss improved from inf to 0.53330, saving model to ./model2/01-0.5333.hdf5\n",
      "20/20 [==============================] - 18s 876ms/step - loss: 0.6563 - acc: 0.6210 - val_loss: 0.5333 - val_acc: 0.7370\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.5009 - acc: 0.7660\n",
      "Epoch 00002: val_loss improved from 0.53330 to 0.48485, saving model to ./model2/02-0.4849.hdf5\n",
      "20/20 [==============================] - 17s 831ms/step - loss: 0.5009 - acc: 0.7660 - val_loss: 0.4849 - val_acc: 0.7710\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.4717 - acc: 0.7860\n",
      "Epoch 00003: val_loss did not improve from 0.48485\n",
      "20/20 [==============================] - 17s 862ms/step - loss: 0.4717 - acc: 0.7860 - val_loss: 0.4974 - val_acc: 0.7850\n",
      "Epoch 1/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.6840 - acc: 0.5730\n",
      "Epoch 00001: val_loss improved from inf to 0.63178, saving model to ./model3/01-0.6318.hdf5\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.6840 - acc: 0.5730 - val_loss: 0.6318 - val_acc: 0.6330\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.5877 - acc: 0.7350\n",
      "Epoch 00002: val_loss improved from 0.63178 to 0.60849, saving model to ./model3/02-0.6085.hdf5\n",
      "20/20 [==============================] - 22s 1s/step - loss: 0.5877 - acc: 0.7350 - val_loss: 0.6085 - val_acc: 0.6595\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.4648 - acc: 0.8515\n",
      "Epoch 00003: val_loss improved from 0.60849 to 0.53652, saving model to ./model3/03-0.5365.hdf5\n",
      "20/20 [==============================] - 23s 1s/step - loss: 0.4648 - acc: 0.8515 - val_loss: 0.5365 - val_acc: 0.7340\n"
     ]
    }
   ],
   "source": [
    "hists = []\n",
    "for i in range(4):\n",
    "    # callback funcs\n",
    "    import datetime\n",
    "    log_dir = \"logs/model%d/fit/\"%i + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    from tensorflow import keras\n",
    "    from time import time\n",
    "    class timeChecker(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.durations = []\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.start = time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.duration = time()-self.start\n",
    "            self.durations.append(self.duration)\n",
    "\n",
    "    tchecker = timeChecker()\n",
    "\n",
    "    from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "    # epoch val_accuracy val_loss\n",
    "\n",
    "    MODEL_DIR = './model%d/'%i\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.mkdir(MODEL_DIR)\n",
    "\n",
    "    modelpath=\"./model%d/{epoch:02d}-{val_loss:.4f}.hdf5\"%i\n",
    "    checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)        \n",
    "\n",
    "\n",
    "    exec('history = model%d.fit(xtr, ytr, batch_size=100, epochs=3, verbose=1,validation_data=(xte, yte) \\\n",
    "               ,callbacks=[tensorboard_callback,tchecker,checkpointer])'%i )\n",
    "    history.history['durations']=tchecker.durations\n",
    "    hists.append(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/modeln/fit --port 6006"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "eda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kera",
   "language": "python",
   "name": "kera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
